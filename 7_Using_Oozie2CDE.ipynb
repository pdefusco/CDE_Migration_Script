{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac7a013-5879-47b4-89f8-c38c86d2ea06",
   "metadata": {},
   "source": [
    "## Using the Oozie2CDE Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572b4d71-ac2d-49be-8686-d6e8449e3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oozie2cde.workflow as wf\n",
    "import oozie2cde.cdejob as cj\n",
    "import oozie2cde.cderesource as rs\n",
    "import os\n",
    "from importlib import reload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9eea1d-cd77-44a8-bc95-ca56cf34b532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'oozie2cde.workflow' from '/home/cdsw/oozie2cde/workflow.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565b1173-170f-4f2e-9f02-efb2cad67a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"oozie_workflows/oozie_hive_workflow_with_properties\"\n",
    "cde_resource_name = \"hadoop2CDE_migration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26147f41-a96b-48bf-b7a0-3f74d3962cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ow = wf.OozieWorkflow(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8d6398-3fe0-47cc-98db-a97dc70b26e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oozie workflow file hive_properties_workflow.xml found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'workflow-app': {'@xmlns': 'uri:oozie:workflow:0.4',\n",
       "  '@name': 'simple-Workflow',\n",
       "  'start': {'@to': 'Create_External_Table'},\n",
       "  'action': [{'@name': 'Create_External_Table',\n",
       "    'hive': {'@xmlns': 'uri:oozie:hive-action:0.4',\n",
       "     'job-tracker': '${jobTracker}',\n",
       "     'name-node': '${nameNode}',\n",
       "     'script': '${script_name_external}'},\n",
       "    'ok': {'@to': 'Create_orc_Table'},\n",
       "    'error': {'@to': 'kill_job'}},\n",
       "   {'@name': 'Create_orc_Table',\n",
       "    'hive': {'@xmlns': 'uri:oozie:hive-action:0.4',\n",
       "     'job-tracker': '${jobTracker}',\n",
       "     'name-node': '${nameNode}',\n",
       "     'script': '${script_name_orc}'},\n",
       "    'ok': {'@to': 'Insert_into_Table'},\n",
       "    'error': {'@to': 'kill_job'}},\n",
       "   {'@name': 'Insert_into_Table',\n",
       "    'hive': {'@xmlns': 'uri:oozie:hive-action:0.4',\n",
       "     'job-tracker': '${jobTracker}',\n",
       "     'name-node': '${nameNode}',\n",
       "     'script': '${script_name_copy}',\n",
       "     'param': '${database}'},\n",
       "    'ok': {'@to': 'end'},\n",
       "    'error': {'@to': 'kill_job'}}],\n",
       "  'kill': {'@name': 'kill_job', 'message': 'Job failed'},\n",
       "  'end': {'@name': 'end'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_d = ow.ooziexml_to_dict()\n",
    "workflow_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9bc8be-8aaa-451d-bef7-825966ffd18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties file job1.properties found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nameNode': 'hdfs://rootname',\n",
       " 'jobTracker': 'xyz.com:8088',\n",
       " 'script_name_external': 'oozie_workflows/oozie_hive_workflow_with_properties/external.hive',\n",
       " 'script_name_orc': 'oozie_workflows/oozie_hive_workflow_with_properties/orc.hive',\n",
       " 'script_name_copy': 'oozie_workflows/oozie_hive_workflow_with_properties/Copydata.hive',\n",
       " 'database_name': 'default'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties_dict = ow.parse_workflow_properties()\n",
    "properties_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da28904-76da-426e-a1de-26208f19d5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workflow-app': {'@xmlns': 'uri:oozie:workflow:0.4',\n",
       "  '@name': 'simple-Workflow',\n",
       "  'start': {'@to': 'Create_External_Table'},\n",
       "  'action': [{'@name': 'Create_External_Table',\n",
       "    'hive': {'@xmlns': 'uri:oozie:hive-action:0.4',\n",
       "     'job-tracker': 'xyz.com:8088',\n",
       "     'name-node': 'hdfs://rootname',\n",
       "     'script': 'oozie_workflows/oozie_hive_workflow_with_properties/external.hive'},\n",
       "    'ok': {'@to': 'Create_orc_Table'},\n",
       "    'error': {'@to': 'kill_job'}},\n",
       "   {'@name': 'Create_orc_Table',\n",
       "    'hive': {'@xmlns': 'uri:oozie:hive-action:0.4',\n",
       "     'job-tracker': 'xyz.com:8088',\n",
       "     'name-node': 'hdfs://rootname',\n",
       "     'script': 'oozie_workflows/oozie_hive_workflow_with_properties/orc.hive'},\n",
       "    'ok': {'@to': 'Insert_into_Table'},\n",
       "    'error': {'@to': 'kill_job'}},\n",
       "   {'@name': 'Insert_into_Table',\n",
       "    'hive': {'@xmlns': 'uri:oozie:hive-action:0.4',\n",
       "     'job-tracker': 'xyz.com:8088',\n",
       "     'name-node': 'hdfs://rootname',\n",
       "     'script': 'oozie_workflows/oozie_hive_workflow_with_properties/Copydata.hive',\n",
       "     'param': 'database'},\n",
       "    'ok': {'@to': 'end'},\n",
       "    'error': {'@to': 'kill_job'}}],\n",
       "  'kill': {'@name': 'kill_job', 'message': 'Job failed'},\n",
       "  'end': {'@name': 'end'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_d_props = ow.workflow_properties_lookup(workflow_d, properties_dict)\n",
    "workflow_d_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884b3ca0-acca-4654-98fd-4ec5cee71a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Editing Hive Query File: orc.hive\n",
      "\n",
      "The input Hive query is: \n",
      "\n",
      "Create Table orc_table(\n",
      "name string, -- Concate value of first name and last name with space as seperator\n",
      "yearofbirth int,\n",
      "age int, -- Current year minus year of birth\n",
      "address string,\n",
      "zip int\n",
      ")\n",
      "STORED AS ORC\n",
      ";\n",
      "\n",
      "The output Hive query is: \n",
      "\n",
      "Create Table orc_table(\n",
      "name string, -- Concate value of first name and last name with space as seperator\n",
      "yearofbirth int,\n",
      "age int, -- Current year minus year of birth\n",
      "address string,\n",
      "zip int\n",
      ")\n",
      "STORED AS ORC\n",
      ";\n",
      "\n",
      "Editing Hive Query File: external.hive\n",
      "\n",
      "The input Hive query is: \n",
      "\n",
      "Create external table external_table(\n",
      "name string,\n",
      "age int,\n",
      "address string,\n",
      "zip int\n",
      ")\n",
      "row format delimited\n",
      "fields terminated by ','\n",
      "stored as textfile\n",
      "location '/test/abc';\n",
      "\n",
      "The output Hive query is: \n",
      "\n",
      "Create external table external_table(\n",
      "name string,\n",
      "age int,\n",
      "address string,\n",
      "zip int\n",
      ")\n",
      "row format delimited\n",
      "fields terminated by ','\n",
      "stored as textfile\n",
      "location '/test/abc';\n",
      "\n",
      "Editing Hive Query File: Copydata.hive\n",
      "\n",
      "The input Hive query is: \n",
      "\n",
      "use \"default\"; -- input from Oozie\n",
      "insert into table orc_table\n",
      "select\n",
      "concat(first_name,' ',last_name) as name,\n",
      "yearofbirth,\n",
      "year(from_unixtime) --yearofbirth as age,\n",
      "address,\n",
      "zip\n",
      "from external_table\n",
      ";\n",
      "\n",
      "The output Hive query is: \n",
      "\n",
      "use \"default\"; -- input from Oozie\n",
      "insert into table orc_table\n",
      "select\n",
      "concat(first_name,' ',last_name) as name,\n",
      "yearofbirth,\n",
      "year(from_unixtime) --yearofbirth as age,\n",
      "address,\n",
      "zip\n",
      "from external_table\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "ow.query_properties_lookup(properties_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd728e30-da2f-420d-9177-7b66dccf5ef6",
   "metadata": {},
   "source": [
    "### Converting Oozie Workflows to CDE Payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "da988ce7-9843-42c2-86a4-e517b8d9edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'oozie2cde.cdejob' from '/home/cdsw/oozie2cde/cdejob.py'>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(cj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "688c20d6-8430-435d-895b-075c5e32c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir \"airflow_dags\"\n",
    "dag_dir = \"airflow_dags\"\n",
    "dag_file_name = \"my_dag.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "09af8db7-ccb4-49a5-8215-6586c2d9047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_cde_job = cj.CdeJob(workflow_d_props, \"oozie_migration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7f9eaf42-dc06-43e0-abf4-4d1c2f49a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_cde_job.initialize_dag(dag_dir, dag_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1cbc1e00-dffe-408f-8e5a-64660fdf60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_cde_job.dag_imports(dag_dir, dag_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4cfb718f-033d-4d7e-8551-5226029ac7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_cde_job.dag_declaration(\"pauldefusco\", dag_dir, dag_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c9c7abe3-84fb-4ac0-bd50-5ab23c4a2969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'@name': 'Create_External_Table', 'hive': {'@xmlns': 'uri:oozie:hive-action:0.4', 'job-tracker': 'xyz.com:8088', 'name-node': 'hdfs://rootname', 'script': 'oozie_workflows/oozie_hive_workflow_with_properties/external.hive'}, 'ok': {'@to': 'Create_orc_Table'}, 'error': {'@to': 'kill_job'}}\n",
      "{'@name': 'Create_orc_Table', 'hive': {'@xmlns': 'uri:oozie:hive-action:0.4', 'job-tracker': 'xyz.com:8088', 'name-node': 'hdfs://rootname', 'script': 'oozie_workflows/oozie_hive_workflow_with_properties/orc.hive'}, 'ok': {'@to': 'Insert_into_Table'}, 'error': {'@to': 'kill_job'}}\n",
      "{'@name': 'Insert_into_Table', 'hive': {'@xmlns': 'uri:oozie:hive-action:0.4', 'job-tracker': 'xyz.com:8088', 'name-node': 'hdfs://rootname', 'script': 'oozie_workflows/oozie_hive_workflow_with_properties/Copydata.hive', 'param': 'database'}, 'ok': {'@to': 'end'}, 'error': {'@to': 'kill_job'}}\n"
     ]
    }
   ],
   "source": [
    "cde_payloads = spark_cde_job.parse_oozie_workflow(dag_dir, dag_file_name, workflow_d_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9bfee8fb-42c4-49a4-9c77-c0ae236a714e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cde_payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c939e9-c9a5-4271-985a-fceaf0718eac",
   "metadata": {},
   "source": [
    "##### No Spark CDE Job Payloads were created because the workflow.xml file does not contain spark actions. Let's try again with a workflow containing a spark action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fb4a4b7b-a5e1-4a45-98a2-b891df4ec4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = \"oozie_workflows/spark_oozie_workflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "de25db06-2810-4683-a27d-bb56a50c5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "ow = wf.OozieWorkflow(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8383a3b5-4233-47c5-b470-a9960f44a789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oozie workflow file spark_action_workflow.xml found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'workflow-app': {'@xmlns': 'uri:oozie:workflow:0.5',\n",
       "  '@name': 'SparkWordCount',\n",
       "  'start': {'@to': 'spark-node'},\n",
       "  'action': {'@name': 'spark-node',\n",
       "   'spark': {'@xmlns': 'uri:oozie:spark-action:0.1',\n",
       "    'job-tracker': '${jobTracker}',\n",
       "    'name-node': '${nameNode}',\n",
       "    'prepare': {'delete': {'@path': '${nameNode}/user/${wf:user()}/${examplesRoot}/output-data'}},\n",
       "    'master': '${master}',\n",
       "    'name': 'SparkPi',\n",
       "    'class': 'org.apache.spark.examples.SparkPi',\n",
       "    'jar': 'example_spark_jobs/jobs/pi.scala',\n",
       "    'spark-opts': '--executor-memory 2G --num-executors 5',\n",
       "    'arg': 'value=10'},\n",
       "   'ok': {'@to': 'end'},\n",
       "   'error': {'@to': 'fail'}},\n",
       "  'kill': {'@name': 'fail',\n",
       "   'message': 'Workflow failed, error\\n            message[${wf:errorMessage(wf:lastErrorNode())}]'},\n",
       "  'end': {'@name': 'end'}}}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow_d = ow.ooziexml_to_dict()\n",
    "workflow_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e5eb3e17-5b64-4355-ad3a-df133374f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No properties file found.\n",
      "\n",
      "If properties file is expected, please ensure it is in the workflow directory.\n",
      "\n",
      "If properties file is not expected, please ignore this message.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "properties_dict = ow.parse_workflow_properties()\n",
    "properties_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c906cdc-b72a-4f51-857e-058053aa3000",
   "metadata": {},
   "source": [
    "##### No ancilliary properties file found. We can proceed with the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "ff069484-842b-47ab-a145-4bcc0dfb2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir \"airflow_dags\"\n",
    "dag_dir = \"airflow_dags\"\n",
    "dag_file_name = \"my_spark_dag.py\"\n",
    "dag_name = \"oozie_2_airflow_dag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7cc83b29-706d-48a5-a7c1-5b715e35d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_airflow_job = cj.CdeJob(workflow_d_props, \"oozie_migration\", dag_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ffcc2e57-74b7-49a6-9056-1f9be9c81fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes previously existing DAGs with this file name.\n",
    "cde_airflow_job.initialize_dag(dag_dir, dag_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3b0a3340-5c73-4154-887b-eb4a56d49a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appends DAG import statememts. Run once. If run twice by mistake, rerun dag initialization method. \n",
    "cde_airflow_job.dag_imports(dag_dir, dag_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "22118c52-af67-40ed-8582-7d2b99096097",
   "metadata": {},
   "outputs": [],
   "source": [
    "cde_airflow_job.dag_declaration(\"pauldefusco\", dag_dir, dag_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "15873502-22e0-4941-9d1b-bfd7b2d4f29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Job Name: SparkPi\n",
      "Working on Spark CDE Job: SparkPi\n",
      "Converted Spark Oozie Action into Spark CDE Payload\n"
     ]
    }
   ],
   "source": [
    "spark_cde_payloads = cde_airflow_job.parse_oozie_workflow(dag_dir, dag_file_name, workflow_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6b8ead5f-4185-406e-9b34-2020dcb80cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'SparkPi',\n",
       "  'type': 'spark',\n",
       "  'retentionPolicy': 'keep_indefinitely',\n",
       "  'mounts': [{'dirPrefix': '/', 'resourceName': 'oozie_migration'}],\n",
       "  'spark': {'file': 'pi.scala',\n",
       "   'conf': {'spark.pyspark.python': 'python3'},\n",
       "   'executorMemory': '2G',\n",
       "   'numExecutors': 5},\n",
       "  'schedule': {'enabled': False}}]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_cde_payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4b173-2fe5-4863-886f-f1fefc46f522",
   "metadata": {},
   "source": [
    "##### We should also validate that the Airflow DAG has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "84c32dff-5dc8-4660-bc64-2ae245148e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# The new Airflow DAG\n",
      "from dateutil import parser\n",
      "    \n",
      "from datetime import datetime, timedelta\n",
      "    \n",
      "import pendulum\n",
      "    \n",
      "from airflow import DAG\n",
      "    \n",
      "from airflow.operators.email import EmailOperator\n",
      "    \n",
      "from airflow.operators.python_operator import PythonOperator\n",
      "    \n",
      "from cloudera.cdp.airflow.operators.cdw_operator import CDWOperator\n",
      "    \n",
      "from cloudera.cdp.airflow.operators.cde_operator import CDEJobRunOperator\n",
      "\n",
      "default_args = {\n",
      "        'owner': 'pauldefusco',\n",
      "    'retry_delay': timedelta(seconds=5),\n",
      "    'depends_on_past': False,\n",
      "    'start_date': pendulum.datetime(2016, 1, 1, tz=\"Europe/Amsterdam\")\n",
      "    }\n",
      "\n",
      "oozie_2_airflow_dag = DAG(\n",
      "    'airflow-pipeline-demo',\n",
      "    default_args=default_args,\n",
      "    schedule_interval='@daily',\n",
      "    catchup=False,\n",
      "    is_paused_upon_creation=False\n",
      "    )\n",
      "\n",
      "SparkPi_Step = CDEJobRunOperator(\n",
      "        task_id='SparkPi',\n",
      "        dag=oozie_2_airflow_dag,\n",
      "        job_name='SparkPi'\n",
      "        )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(dag_dir+\"/\"+dag_file_name, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a99ca-bcab-4df3-b4b9-393456c72b43",
   "metadata": {},
   "source": [
    "##### One last thing before we can implement in CDE. We need to create the Payload for the Airflow DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3aecee36-eb4b-41ca-95f6-882f370f6523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Airflow CDE Job: oozie2airflow_dag\n",
      "Converted DAG into Airflow CDE Payload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'airflow',\n",
       " 'airflow': {'dagFile': 'my_spark_dag.py'},\n",
       " 'identity': {'disableRoleProxy': True},\n",
       " 'mounts': [{'dirPrefix': '/', 'resourceName': 'hadoop2CDE_migration'}],\n",
       " 'name': 'oozie2airflow_dag',\n",
       " 'retentionPolicy': 'keep_indefinitely'}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airflow_cde_payload = cde_airflow_job.oozie_to_cde_airflow_payload(dag_file_name, cde_resource_name, \"oozie2airflow_dag\")\n",
    "airflow_cde_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857818b3-ef40-4f9a-b634-c6720e826d30",
   "metadata": {},
   "source": [
    "### Instantiating the Jobs in the CDE Virtual Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "250193e2-f95a-4e98-be71-72b57276f683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'oozie2cde.cderesource' from '/home/cdsw/oozie2cde/cderesource.py'>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "dacb0abb-f1e3-4050-82b6-69616639c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WORKLOAD_USER\"] = \"pauldefusco\"\n",
    "os.environ[\"JOBS_API_URL\"] = \"https://tk5p4pn9.cde-6fr6l74r.go01-dem.ylcu-atmi.cloudera.site/dex/api/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0dfae4ed-080e-4900-af90-68439ba8168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = rs.CdeResource(os.environ[\"JOBS_API_URL\"], os.environ[\"WORKLOAD_USER\"], cde_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d55b4ee4-f766-4c6a-827e-785baf12eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = resource.set_cde_token(os.environ[\"WORKLOAD_PASSWORD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1a29d8d1-f7e7-4067-94d7-46debc15875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409\n",
      "{\"status\":\"error\",\"message\":\"resource with name already exists\"}\n"
     ]
    }
   ],
   "source": [
    "resource.create_cde_resource(token, cde_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5012fbda-aefa-4635-8a6d-7b6e3c3234a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Job: pi.scala\n",
      "Response Status Code 201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resource.upload_file(cde_resource_name, \"example_spark_jobs/jobs\", \"pi.scala\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "ebc73b75-6c90-4c77-88b0-ba119a48793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Job: SparkPi\n",
      "Response Status Code 500\n",
      "{\"status\":\"error\",\"message\":\"job with name already exists\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resource.create_job_from_resource(token, spark_cde_payloads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "802ec9d9-da1a-44bb-81c8-9e324d3ffb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'airflow',\n",
       " 'airflow': {'dagFile': 'my_spark_dag.py'},\n",
       " 'identity': {'disableRoleProxy': True},\n",
       " 'mounts': [{'dirPrefix': '/', 'resourceName': 'hadoop2CDE_migration'}],\n",
       " 'name': 'oozie2airflow_dag',\n",
       " 'retentionPolicy': 'keep_indefinitely'}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airflow_cde_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "d0312162-3227-4849-89e1-58711d8f25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Job: my_spark_dag.py\n",
      "Response Status Code 201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resource.upload_file(cde_resource_name, \"airflow_dags\", \"my_spark_dag.py\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "9d1cba6d-b91c-4821-a8b0-b4b029a06488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Job: oozie2airflow_dag\n",
      "Response Status Code 201\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resource.create_job_from_resource(token, airflow_cde_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39871e2-95e7-4da8-8008-a55a92606d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
